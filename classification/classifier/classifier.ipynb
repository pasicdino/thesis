{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "READ!!!\n",
    "\n",
    "This is a first attempt at a classifier and turned out not to work well at all.\n",
    "\n",
    "It basically converted the instructions, the replaced, and the replacement, into 3 w2v embeddings.\n",
    "Even though it had acceptable scores on the validation set, it worked very very bad on unseen data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m generic \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: literal_eval(x)\n\u001B[0;32m      5\u001B[0m conv \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnutrition\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mingredients\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid_column\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjaccard_similarity\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiff\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecipes\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mingredients_original\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m'\u001B[39m: generic}\n\u001B[1;32m----> 6\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mUsers\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43m01din\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mPycharmProjects\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mthesis\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mcleaned_recipes\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mrecipes_with_JS.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconverters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m df\u001B[38;5;241m.\u001B[39mdrop([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m'\u001B[39m], inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      8\u001B[0m df_ingredients \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m01din\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mthesis\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mingredients\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mingredients_nutrition.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    610\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 611\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1771\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m     (\n\u001B[0;32m   1775\u001B[0m         index,\n\u001B[0;32m   1776\u001B[0m         columns,\n\u001B[0;32m   1777\u001B[0m         col_dict,\n\u001B[1;32m-> 1778\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1779\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1780\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1782\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 230\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    232\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1016\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2027\u001B[0m, in \u001B[0;36mpandas._libs.parsers._apply_converter\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mast\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m literal_eval\n\u001B[0;32m      3\u001B[0m survey_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m01din\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mthesis\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msurvey_results\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msurvey_results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m generic \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mliteral_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m conv \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnutrition\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mingredients\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid_column\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjaccard_similarity\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiff\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecipes\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mingredients_original\u001B[39m\u001B[38;5;124m'\u001B[39m : generic, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m'\u001B[39m: generic}\n\u001B[0;32m      6\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m01din\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mthesis\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcleaned_recipes\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mrecipes_with_JS.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, converters\u001B[38;5;241m=\u001B[39mconv)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ast.py:59\u001B[0m, in \u001B[0;36mliteral_eval\u001B[1;34m(node_or_string)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03mSafely evaluate an expression node or a string containing a Python\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03mexpression.  The string or node provided may only consist of the following\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03mPython literal structures: strings, bytes, numbers, tuples, lists, dicts,\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03msets, booleans, and None.\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node_or_string, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m---> 59\u001B[0m     node_or_string \u001B[38;5;241m=\u001B[39m \u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_or_string\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meval\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node_or_string, Expression):\n\u001B[0;32m     61\u001B[0m     node_or_string \u001B[38;5;241m=\u001B[39m node_or_string\u001B[38;5;241m.\u001B[39mbody\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "survey_df = pd.read_csv(r'C:\\Users\\01din\\PycharmProjects\\thesis\\data\\survey_results\\survey_results.csv')\n",
    "generic = lambda x: literal_eval(x)\n",
    "conv = {'nutrition' : generic, 'steps' : generic, 'ingredients' : generic, 'id_column' : generic, 'jaccard_similarity' : generic, 'diff' : generic, 'recipes' : generic, 'ingredients_original' : generic, 'tags': generic}\n",
    "df = pd.read_csv(r\"C:\\Users\\01din\\PycharmProjects\\thesis\\data\\cleaned_recipes\\recipes_with_JS.csv\", converters=conv)\n",
    "df.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "df_ingredients = pd.read_csv(r\"C:\\Users\\01din\\PycharmProjects\\thesis\\data\\ingredients\\ingredients_nutrition.csv\")\n",
    "df_ingredients.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "df_ingredients['nutrition'] = df_ingredients['nutrition'].apply(lambda x: literal_eval(x) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ingredients = df_ingredients.ingredient.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['ingredients_original'] = df['ingredients_original'].apply(lambda x: ['_'.join(i.split()) for i in x])\n",
    "df['steps'] = df['steps'].apply(lambda x: [f'<STEP> {s}' for s in x])\n",
    "df['text'] = df.apply(lambda row: ' <INGR> '.join(row['ingredients_original']) + ' ' + ' '.join(row['steps']), axis=1)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare the corpus\n",
    "corpus = df['text'].str.split().tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(corpus, window=5, min_count=1, workers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.text.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model.save(r\"C:\\Users\\01din\\PycharmProjects\\thesis\\models\\w2v\\recipe_word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.68      0.69      0.69        52\n",
      "         Yes       0.69      0.67      0.68        52\n",
      "\n",
      "    accuracy                           0.68       104\n",
      "   macro avg       0.68      0.68      0.68       104\n",
      "weighted avg       0.68      0.68      0.68       104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "35 fits failed out of a total of 70.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\01din\\PycharmProjects\\thesis\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.71138841        nan 0.71864601        nan 0.70674125\n",
      "        nan 0.70192197        nan 0.68749283        nan 0.68984509\n",
      "        nan 0.68984509]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec = Word2Vec.load(r'C:\\Users\\01din\\PycharmProjects\\thesis\\models\\w2v\\recipe_word2vec.model')\n",
    "\n",
    "# Function that calculates the average Word2Vec vector for a given piece of text\n",
    "def average_word2vec(text, word2vec_model):\n",
    "    words = text.split()\n",
    "    word_vecs = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if not word_vecs:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "# Calculate the average Word2Vec vectors for instructions, replaced, and replacement separately\n",
    "df['instructions_vec'] = df['steps'].apply(lambda x: ' '.join(x)).apply(average_word2vec, args=(word2vec,))\n",
    "survey_df['replaced_vec'] = survey_df['replaced'].apply(average_word2vec, args=(word2vec,))\n",
    "survey_df['replacement_vec'] = survey_df['replacement'].apply(average_word2vec, args=(word2vec,))\n",
    "\n",
    "# Concatenate the vectors\n",
    "survey_df['concat_vec'] = survey_df.apply(lambda row: np.concatenate([row['replaced_vec'], row['replacement_vec'], df.loc[df['id'] == row['id'], 'instructions_vec'].values[0]]), axis=1)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(survey_df['concat_vec'].tolist(), survey_df['result'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameters and the values to test\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # inverse of regularization strength\n",
    "    'penalty': ['l1', 'l2']  # l1 is Lasso, l2 is Ridge\n",
    "}\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = make_scorer(accuracy_score)\n",
    "\n",
    "# Define the cross-validation grid search\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=scoring)\n",
    "\n",
    "# Convert the list of vectors to a 2D array, which is necessary for sklearn\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train_array, y_train_array)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initialize and train the classifier using the best parameters\n",
    "clf_best = LogisticRegression(C=best_params['C'], penalty=best_params['penalty'])\n",
    "clf_best.fit(X_train_array, y_train_array)\n",
    "\n",
    "# Convert the list of test vectors to a 2D array\n",
    "X_test_array = np.array(X_test)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf_best.predict(X_test_array)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "clf = clf_best"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No']\n"
     ]
    }
   ],
   "source": [
    "def classify(input_data, word2vec_model, clf):\n",
    "    # Compute the average Word2Vec vectors\n",
    "    instructions_vec = average_word2vec(' '.join(input_data['instructions']), word2vec_model)\n",
    "    replaced_vec = average_word2vec(input_data['replaced'], word2vec_model)\n",
    "    replacement_vec = average_word2vec(input_data['replacement'], word2vec_model)\n",
    "\n",
    "    # Concatenate the vectors\n",
    "    concat_vec = np.concatenate([replaced_vec, replacement_vec, instructions_vec])\n",
    "\n",
    "    # Reshape the vector to match the input shape of the classifier\n",
    "    input_vector = concat_vec.reshape(1, -1)\n",
    "\n",
    "    # Predict the class\n",
    "    prediction = clf.predict(input_vector)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Now you can classify a new instance like this:\n",
    "input_data = {\n",
    "    'instructions': df.iloc[0].steps,\n",
    "    'replaced': 'yellow onions',\n",
    "    'replacement': 'white onions'\n",
    "}\n",
    "result = classify(input_data, word2vec, clf)\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rump_roast', 0.3182893719958118, 'No'), ('cut_green_beans', 0.3149857359418725, 'No'), ('ground_venison', 0.3112943263634743, 'No'), ('hash_brown_potatoes', 0.3103270440281583, 'No'), ('lean_hamburger', 0.3097080993136041, 'No'), ('chuck_roast', 0.30927411343386313, 'No'), ('hamburger_meat', 0.3057874812346893, 'No'), ('ground_meat', 0.304608305804419, 'No'), ('v8_vegetable_juice', 0.30360360343243814, 'No'), ('bulk_italian_sausage', 0.3035349714686171, 'No'), ('ground_turkey_breast', 0.30245875209819983, 'No'), ('ground_sirloin', 0.2986165085000022, 'No'), ('ground_sausage', 0.2951283021459221, 'No'), ('lima_beans', 0.2947509172598281, 'No'), ('ground_round', 0.29239642895957396, 'No'), ('condensed_tomato_soup', 0.285662723150029, 'No'), ('lean_ground_turkey', 0.2851395617055952, 'No'), ('bulk_pork_sausage', 0.2838574873165043, 'No'), ('pork_sausage', 0.27845599810696564, 'No'), ('extra_lean_ground_beef', 0.2779598713018591, 'No'), ('italian_sausage', 0.26129130009537127, 'No'), ('bell_pepper', 0.2600490353656233, 'No'), ('ground_chicken', 0.25986400018643463, 'No'), ('ground_chuck', 0.2592348767595595, 'No'), ('tomato_soup', 0.24634419353116987, 'No'), ('ground_turkey', 0.23202367327860057, 'No'), ('tomato_sauce', 0.2293647817245624, 'No'), ('ground_pork', 0.2182598000811375, 'No'), ('green_pepper', 0.15850447491242706, 'No'), ('lean_ground_beef', 0.15457147115209957, 'No')]\n"
     ]
    }
   ],
   "source": [
    "def get_matching_ingredients(df, ingredient):\n",
    "    # Get the 'FdGrp_Desc' of the specific ingredient\n",
    "    ingredient_grp = df[df['ingredient'] == ingredient]['FdGrp_Desc'].values[0]\n",
    "\n",
    "    # Get all ingredients from the same group\n",
    "    matching_ingredients = df[df['FdGrp_Desc'] == ingredient_grp]\n",
    "\n",
    "    return matching_ingredients\n",
    "\n",
    "\n",
    "def rank_replacements(instructions, replaced, possible_replacements, word2vec_model, clf):\n",
    "    # possible_replacements = get_matching_ingredients(possible_replacements, replaced).ingredient.tolist()\n",
    "    similar_words = word2vec_model.wv.most_similar(replaced, topn=30)\n",
    "    possible_replacements = [word for word, similarity in similar_words]\n",
    "    scores = []\n",
    "    for replacement in possible_replacements:\n",
    "        # Compute the feature vector for the replacement\n",
    "        input_data = {\n",
    "            'instructions': instructions,\n",
    "            'replaced': replaced,\n",
    "            'replacement': replacement\n",
    "        }\n",
    "        concat_vec = average_word2vec(' '.join(input_data['instructions']), word2vec_model)\n",
    "        replaced_vec = average_word2vec(input_data['replaced'], word2vec_model)\n",
    "        replacement_vec = average_word2vec(input_data['replacement'], word2vec_model)\n",
    "        feature_vector = np.concatenate([replaced_vec, replacement_vec, concat_vec]).reshape(1, -1)\n",
    "\n",
    "        # Get the classifier's confidence score for the replacement\n",
    "        confidence_score = clf.predict_proba(feature_vector)[0][1]\n",
    "        predicted_class = clf.predict(feature_vector)[0]\n",
    "        scores.append((replacement, confidence_score, predicted_class))\n",
    "\n",
    "    # Sort the replacements by the confidence scores in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return scores\n",
    "print(rank_replacements(df.iloc[0].steps, 'ground_beef', df_ingredients, word2vec, clf))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "id                                                                 112140\nname                                            all in the kitchen  chili\nn_ingredients                                                          13\ningredients_original    [ground_beef, yellow_onions, diced_tomatoes, t...\nn_steps                                                                 6\nsteps                   [<STEP> <STEP> <STEP> brown ground beef in lar...\ntags                    [time-to-make, course, preparation, main-dish,...\nminutes                                                               130\ndescription             this modified version of 'mom's' chili was a h...\nnutrition                      [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]\ningredients             [Beef,_ground,_70%_lean_meat_/_30%_fat,_raw, O...\ncontributor_id                                                     196586\nsubmitted                                                      25/02/2005\nsimilar                                                                []\ncombined                <name> all in the kitchen  chili <ingredients>...\ntokenized               ['name', 'all', 'in', 'the', 'kitchen', 'chili...\ninstructions_vec        [-0.58804154, -0.12976503, 0.5716957, 0.174630...\ntext                    ground_beef <INGR> yellow_onions <INGR> diced_...\nName: 0, dtype: object"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "similar_words = word2vec.wv.most_similar('ground_beef', topn=30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
